{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DDPG using a Jupyter notebook\n",
    "- Easy access with Google Colabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from model.DDPG import DDPGAgent\n",
    "from model.single_sea_env import SingleSEAEnv\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "matplotlib.use('Agg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(agent, path=\"./\"):\n",
    "    checkpoint_files = [f for f in os.listdir(path) if f.startswith(\"ddpg_models_episode\")]\n",
    "    \n",
    "    # If there are any checkpoint files, find the latest one\n",
    "    if checkpoint_files:\n",
    "        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        with open(os.path.join(path, latest_checkpoint), \"rb\") as f:\n",
    "            agent = pickle.load(f)\n",
    "        # Extract episode number from filename and continue from the next episode\n",
    "        start_episode = int(latest_checkpoint.split('_')[-1].split('.')[0]) + 1\n",
    "        print(f\"Resuming from episode {start_episode} (loaded {latest_checkpoint})\")\n",
    "    else:\n",
    "        start_episode = 0\n",
    "        print(\"No checkpoints found. Starting training from scratch.\")\n",
    "    \n",
    "    return agent, start_episode\n",
    "\n",
    "def compute_moving_average(data, window_size):\n",
    "    data = np.array(data).flatten()\n",
    "    if len(data) < window_size:\n",
    "        return float(np.mean(data))  # If not enough data, just use the mean\n",
    "    return float(np.convolve(data, np.ones(window_size)/window_size, mode='valid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize environments\n",
    "- SEA gym environment\n",
    "- DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize your custom Gym environment\n",
    "env = SingleSEAEnv(visualize=False, mse_threshold=0.01)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_limit = env.action_space.high[0]  # Assuming symmetric action bounds\n",
    "\n",
    "# Initialize DDPG agent\n",
    "agent = DDPGAgent(state_dim, action_dim, action_limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "smoothed_rewards_plot = []\n",
    "smoothed_rewards = []\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 5001\n",
    "max_steps = 4000  # Maximum steps per episode\n",
    "noise_scale = 0.1  # Initial noise scale\n",
    "\n",
    "# Load the latest checkpoint if available\n",
    "# agent, start_episode = load_latest_checkpoint(agent)\n",
    "start_episode = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for episode in range(start_episode, num_episodes):\n",
    "    state = env.reset()\n",
    "    agent.noise.reset()  # Reset noise for each episode\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action from the agent\n",
    "        action = agent.act(state, noise_scale=noise_scale)\n",
    "\n",
    "        # Step the environment with the selected action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Store experience in replay buffer\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n",
    "    rewards.append(episode_reward)\n",
    "    smoothed_rewards = compute_moving_average(rewards, 200)\n",
    "    # if len(smoothed_rewards) > 0:\n",
    "    #     smoothed_rewards_plot.append(smoothed_rewards[-1])\n",
    "    smoothed_rewards_plot.append(smoothed_rewards)\n",
    "\n",
    "    # Decay noise scale over time for less exploration as training progresses\n",
    "    noise_scale = max(noise_scale * 0.99, 0.01)  # Gradually reduce noise\n",
    "\n",
    "    # Save model every 100 episodes\n",
    "    if episode % 1000 == 0 and episode != 0:\n",
    "        print(\"Saving models at episode:\", episode)\n",
    "        with open(f\"ddpg_models_episode_{episode}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent, f)  # Save the entire agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, label='Episode Reward')\n",
    "plt.plot(range(len(rewards)), smoothed_rewards_plot, label='Smoothed Reward', color='red', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Rewards over Episodes')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(\"rewards_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Close the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
